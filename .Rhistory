na_count$prop <- round(na_count$na_count/ nrow(data) *100, 1)
na_count
data
data %>%
group_by(year) %>%
summarise(sum(is.na()))
data %>%
group_by(year) %>%
summarise_all(sum(is.na(.)))
data %>%
group_by(year) %>%
summarise_all(function(y) sum(length(which(is.na(y)))))
require("data.table")
require("randomForest")
library(tidyverse)
library(cluster)    # clustering algorithms
library(factoextra) # clustering visualization
library(tidyverse)
library(VIM)
#wd
setwd('/home/mateo1/repos/financialinclusionClustering')
data = read.csv("data/data.csv")
data = data[ data$year == 2017,]
# imputacion hot deck
hotdeck_imp <-hotdeck(data  , imp_var =F)
hotdeck_imp$imputer = 'HotDeck'
# levanto data from py
data_imp = read.csv('results/data_imputada_py.csv') %>%
bind_rows(hotdeck_imp) %>%
bind_rows(data %>%
na.omit() %>%
mutate(imputer= 'Complete'))
data_imp
write.csv(data_imp, file = "results/data_imputada.csv")
table(data_imp$imputer)
for (i in names(data_imp[, 4:ncol(data_imp) ])) {
print(i)
if (i != 'imputer'){
print ( data_imp %>%
ggplot(aes(x = data_imp[,i], y = imputer, fill = imputer )) +
geom_density_ridges(
scale = 2,
quantiles = 4,
quantile_lines = TRUE,   vline_size = 0.5, vline_color = "red",
alpha = .5
, jittered_points = TRUE, point_alpha = 0.3
) +
# scale_point_color_hue(l = 40) +
# scale_discrete_manual(aesthetics = "point_shape", values = c(21, 22, 23, 24, 25))+
labs()+
theme(legend.position = 'none',
axis.title = element_blank())
)
ggsave(paste0('results/dist_imp/dist_',i,'.jpg' ))
}
}
require("data.table")
require("randomForest")
library(tidyverse)
library(cluster)    # clustering algorithms
library(factoextra) # clustering visualization
#limpio la memoria
rm( list=ls() )  #remove all objects
gc()             #garbage collection
setwd('/home/mateo1/repos/financialinclusionClustering')
#leo el dataset , aqui se puede usar algun super dataset con Feature Engineering
dataset <- read.csv('results/data_imputada.csv')
glimpse(dataset)
data = dataset[, 3:ncol(dataset) ]
index = dataset[, 1:3]
#quito los nulos para que se pueda ejecutar randomForest,  Dios que algoritmo prehistorico ...
# dataset  <- na.roughfix( dataset )
# dataset  <- na.omit(dataset)
gc()
rf_list = list()
dist_list = list()
hclust_list = list()
sil_list = list()
df_sil <- data.frame(matrix(ncol = 3, nrow = 0))
colnames(df_sil) <- c("imputer", "ncluster", "avg_sil")
for(imputer_i in unique(data$imputer)){
print(imputer_i)
modelo  <- randomForest( x= data %>%
filter(imputer== imputer_i)  %>%
select(-imputer)
,
y= NULL,
ntree= 100, #se puede aumentar a 10000
proximity= TRUE,
oob.prox = TRUE )
#genero los clusters jerarquicos
dist_mtrx <-  as.dist ( 1.0 - modelo$proximity) #distancia = 1.0 - proximidad
hclust.rf  <- hclust( dist_mtrx,
method= "ward.D2" )
for (nclust in c(2:15)){
print(nclust)
rf.cluster  <- cutree( hclust.rf, nclust) #corto los arboles
sil <- silhouette (rf.cluster,dist_mtrx) # or use your cluster vector
sil_avg <- summary(sil)[[4]]
row <- c("imputer" = imputer_i, "ncluster"=nclust, "avg_sil" = sil_avg)
df_sil <-  rbind(df_sil, data.frame(t(row)) )
}
rf_list[[imputer_i]] = modelo
dist_list[[imputer_i]] = dist_mtrx
hclust_list[[imputer_i]] = hclust.rf
# sil_list[[imputer_i]] = df_sil
# df_sil <- data.frame(matrix(ncol = 3, nrow = 0))
# colnames(df_sil) <- c("imputer", "ncluster", "avg_sil")
}
#grafico silhoutte
df_sil %>%
mutate_at(.vars = c("ncluster", "avg_sil"), .funs = as.double) %>%
ggplot(aes(ncluster, avg_sil, color = imputer)) +
geom_line()+ geom_point()+
theme(legend.position = 'bottom')+
labs(y = 'Silhouette promedio', x = 'Cantidad de clústers')
ggsave(filename = 'results/sil_avg.jpg')
dist_list
hclust_list
gc()
library(tidyverse)
library(ggplot2)
library(GGally)
library(VIM)
library(mice) # Cargamos la librería
#wd
setwd('/home/mateo1/repos/financialinclusionClustering')
#data
data = read.csv("data/data.csv")
data
### EDA ###
countries = pd.read_csv("data/FINDEXCountry.csv")
countries
### EDA ###
countries = read_csv("data/FINDEXCountry.csv")
countries
data <- data %>%
left_join(countries %>%
select("Country Code", "Region"))
data
data <- data %>%
left_join(countries %>%
select("Country Code", "Region"),
by = c("Country.Code" = "Country Code"))
data
#plot
ggpairs(data=data, columns = 4:ncol(data) , ggplot2::aes(colour=Country.Code))
#plot
ggpairs(data=data, columns = 4:ncol(data) , ggplot2::aes(colour=Country.Code))
#plot
ggpairs(data=complete.cases(data), columns = 4:ncol(data) , ggplot2::aes(colour=Country.Code))
complete.cases(data)
#plot
ggpairs(data=data[complete.cases(data)], columns = 4:ncol(data) , ggplot2::aes(colour=Country.Code))
data[complete.cases(data)]
data[complete.cases(data),]
#plot
ggpairs(data=data[complete.cases(data),], columns = 4:ncol(data) , ggplot2::aes(colour=Country.Code))
#plot
ggpairs(data=data %>% drop_na(), columns = 4:ncol(data) , ggplot2::aes(colour=Country.Code))
data
data <- data %>%
right_join(countries %>%
select("Country Code", "Region"),
by = c("Country.Code" = "Country Code"))
data
#data
data = read.csv("data/data.csv")
data <- data %>%
right_join(countries %>%
select("Country Code", "Region"),
by = c("Country.Code" = "Country Code"))
data
data <- data %>%
right_join(countries %>%
select("Country Code", "Region"),
by = c("Country.Code" = "Country Code")) %>%
select(year, Country.Code, Country.Name, Region, everything(.))
### EDA ###
countries = read_csv("data/FINDEXCountry.csv")
data <- data %>%
right_join(countries %>%
select("Country Code", "Region"),
by = c("Country.Code" = "Country Code")) %>%
select(year, Country.Code, Country.Name, Region, everything(.))
#data
data = read.csv("data/data.csv")
countries
data %>%
right_join(countries %>%
select("Country Code", "Region"),
by = c("Country.Code" = "Country Code"))
data <- data %>%
right_join(countries %>%
select("Country Code", "Region"),
by = c("Country.Code" = "Country Code")) %>%
select(year, Country.Code, Country.Name, Region, everything(.))
data
#plot
ggpairs(data=data %>% drop_na(), columns = 5:ncol(data) , ggplot2::aes(colour=Country.Code))
table(data$Region)
data %>% drop_na() %>% table(Region)
data %>% drop_na()
data_comple = data %>% drop_na()
table(data_comple$Region)
#plot
ggpairs(data=data %>% drop_na(), columns = 5:ncol(data) , ggplot2::aes(colour=Country.Code))
warnings()
data %>%
select_if(is_numeric)
data %>%
select_if(is_numeric) %>%
select(-year)
# PCA
#comando que ejecuta el metodo de componentes principales
datos_pca = prcomp(data %>%
select_if(is_numeric) %>%
select(-year),
scale = TRUE)
# PCA
#comando que ejecuta el metodo de componentes principales
datos_pca = prcomp(data %>%
drop_na() %>%
select_if(is_numeric) %>%
select(-year),
scale = TRUE)
datos_pca
# PCA
#comando que ejecuta el metodo de componentes principales
datos_compl = data %>%
drop_na()
datos_pca = prcomp(datos_compl %>%
select_if(is_numeric) %>%
select(-year),
scale = TRUE)
ggbiplot(datos_pca, obs.scale=1,groups = factor(datos_compl$Region) )
library(ggbiplot)
install_github("vqv/ggbiplot")
library(devtools)
install_github("vqv/ggbiplot")
#install_github("vqv/ggbiplot")
library(ggbiplot)
ggbiplot(datos_pca, obs.scale=1,groups = factor(datos_compl$Region) )
ggbiplot(datos_pca, obs.scale=1,groups = factor(datos_compl$Region) ) +
theme(legend.position = "bottom")
datos_compl
datos_pca = prcomp(datos_compl %>%
filter(year == 2017) %>%
select_if(is_numeric) %>%
select(-year),
scale = TRUE)
ggbiplot(datos_pca, obs.scale=1,groups = factor(datos_compl$Region) ) +
theme(legend.position = "bottom")
# PCA
#comando que ejecuta el metodo de componentes principales
datos_compl = data %>%
drop_na() %>%
filter(year == 2017)
datos_pca = prcomp(datos_compl %>%
select_if(is_numeric) %>%
select(-year),
scale = TRUE)
ggbiplot(datos_pca, obs.scale=1,groups = factor(datos_compl$Region) ) +
theme(legend.position = "bottom")
datos_pca
#loadings
carga1 = data.frame(cbind(X=1:length(datos_compl),
primeracarga=data.frame(datos_pca$rotation)[,1]))
carga1
datos_compl
#loadings
carga1 = data.frame(cbind(X=1:length(datos_compl %>%
select_if(is_numeric) %>%
select(-year)),
primeracarga=data.frame(datos_pca$rotation)[,1]))
carga1
carga2 = data.frame(cbind(X=1:length(datos_compl %>%
select_if(is_numeric) %>%
select(-year)),
segundacarga=data.frame(datos_pca$rotation)[,2]))
cbind(carga1,carga2)
ggplot(carga1, aes(X,primeracarga) ,
fill=tramo ) + geom_bar ( stat="identity" ,
position="dodge" ,
fill ="royalblue" ,
width =0.5 ) + xlab( 'Tramo' ) + ylab('Primeracarga ' )
ggplot( carga2 , aes ( X , segundacarga ) ,
fill =X ) + geom_bar ( stat="identity" , position="dodge" ,
fill ="royalblue" ,
width =0.5 ) +
xlab('Tramo') + ylab('Segundacarga')
datos_pca
corr_coph = list()
require("data.table")
require("randomForest")
library(tidyverse)
library(cluster)    # clustering algorithms
library(factoextra) # clustering visualization
#limpio la memoria
rm( list=ls() )  #remove all objects
gc()             #garbage collection
setwd('/home/mateo1/repos/financialinclusionClustering')
#leo el dataset , aqui se puede usar algun super dataset con Feature Engineering
dataset <- read.csv('results/data_imputada.csv')
glimpse(dataset)
data = dataset[, 3:ncol(dataset) ]
index = dataset[, 1:3]
#quito los nulos para que se pueda ejecutar randomForest,  Dios que algoritmo prehistorico ...
# dataset  <- na.roughfix( dataset )
# dataset  <- na.omit(dataset)
gc()
rf_list = list()
dist_list = list()
hclust_list = list()
sil_list = list()
corr_coph = list()
df_sil <- data.frame(matrix(ncol = 3, nrow = 0))
colnames(df_sil) <- c("imputer", "ncluster", "avg_sil")
for(imputer_i in unique(data$imputer)){
print(imputer_i)
modelo  <- randomForest( x= data %>%
filter(imputer== imputer_i)  %>%
select(-imputer)
,
y= NULL,
ntree= 100, #se puede aumentar a 10000
proximity= TRUE,
oob.prox = TRUE )
#genero los clusters jerarquicos
dist_mtrx <-  as.dist ( 1.0 - modelo$proximity) #distancia = 1.0 - proximidad
hclust.rf  <- hclust( dist_mtrx,
method= "ward.D2" )
for (nclust in c(2:15)){
print(nclust)
rf.cluster  <- cutree( hclust.rf, nclust) #corto los arboles
sil <- silhouette (rf.cluster,dist_mtrx) # or use your cluster vector
sil_avg <- summary(sil)[[4]]
row <- c("imputer" = imputer_i, "ncluster"=nclust, "avg_sil" = sil_avg)
df_sil <-  rbind(df_sil, data.frame(t(row)) )
}
rf_list[[imputer_i]] = modelo
dist_list[[imputer_i]] = dist_mtrx
hclust_list[[imputer_i]] = hclust.rf
corr_coph[[imputer_i]] = cor(cophenetic(hclust.rf), dist_mtrx)
# sil_list[[imputer_i]] = df_sil
# df_sil <- data.frame(matrix(ncol = 3, nrow = 0))
# colnames(df_sil) <- c("imputer", "ncluster", "avg_sil")
}
corr_coph
df_corr_coph <- data.frame(x1 = numeric(0),    # Create empty data frame
x2 = numeric(0) )
df_corr_coph
df_corr_coph <- data.frame(x1 = str(0),    # Create empty data frame
x2 = numeric(0) )
df_corr_coph
df_corr_coph <- data.frame(imputer = str(0),    # Create empty data frame
corr_coph = numeric(0) )
df_corr_coph
i = 1
df_sil <- data.frame(matrix(ncol = 3, nrow = 0))
colnames(df_sil) <- c("imputer", "ncluster", "avg_sil")
for(imputer_i in unique(data$imputer)){
print(imputer_i)
modelo  <- randomForest( x= data %>%
filter(imputer== imputer_i)  %>%
select(-imputer)
,
y= NULL,
ntree= 100, #se puede aumentar a 10000
proximity= TRUE,
oob.prox = TRUE )
#genero los clusters jerarquicos
dist_mtrx <-  as.dist ( 1.0 - modelo$proximity) #distancia = 1.0 - proximidad
hclust.rf  <- hclust( dist_mtrx,
method= "ward.D2" )
for (nclust in c(2:15)){
print(nclust)
rf.cluster  <- cutree( hclust.rf, nclust) #corto los arboles
sil <- silhouette (rf.cluster,dist_mtrx) # or use your cluster vector
sil_avg <- summary(sil)[[4]]
row <- c("imputer" = imputer_i, "ncluster"=nclust, "avg_sil" = sil_avg)
df_sil <-  rbind(df_sil, data.frame(t(row)) )
}
rf_list[[imputer_i]] = modelo
dist_list[[imputer_i]] = dist_mtrx
hclust_list[[imputer_i]] = hclust.rf
corr_coph[[imputer_i]] = cor(cophenetic(hclust.rf), dist_mtrx)
# sil_list[[imputer_i]] = df_sil
df_corr_coph[i, ] <- c(imputer_i  , cor(cophenetic(hclust.rf), dist_mtrx) )
# df_sil <- data.frame(matrix(ncol = 3, nrow = 0))
# colnames(df_sil) <- c("imputer", "ncluster", "avg_sil")
}
require("data.table")
require("randomForest")
library(tidyverse)
library(cluster)    # clustering algorithms
library(factoextra) # clustering visualization
#limpio la memoria
rm( list=ls() )  #remove all objects
gc()             #garbage collection
setwd('/home/mateo1/repos/financialinclusionClustering')
#leo el dataset , aqui se puede usar algun super dataset con Feature Engineering
dataset <- read.csv('results/data_imputada.csv')
glimpse(dataset)
data = dataset[, 3:ncol(dataset) ]
index = dataset[, 1:3]
#quito los nulos para que se pueda ejecutar randomForest,  Dios que algoritmo prehistorico ...
# dataset  <- na.roughfix( dataset )
# dataset  <- na.omit(dataset)
gc()
rf_list = list()
dist_list = list()
hclust_list = list()
sil_list = list()
corr_coph = list()
df_corr_coph <- data.frame(imputer = str(0),    # Create empty data frame
corr_coph = numeric(0) )
i = 1
df_sil <- data.frame(matrix(ncol = 3, nrow = 0))
colnames(df_sil) <- c("imputer", "ncluster", "avg_sil")
for(imputer_i in unique(data$imputer)){
print(imputer_i)
modelo  <- randomForest( x= data %>%
filter(imputer== imputer_i)  %>%
select(-imputer)
,
y= NULL,
ntree= 100, #se puede aumentar a 10000
proximity= TRUE,
oob.prox = TRUE )
#genero los clusters jerarquicos
dist_mtrx <-  as.dist ( 1.0 - modelo$proximity) #distancia = 1.0 - proximidad
hclust.rf  <- hclust( dist_mtrx,
method= "ward.D2" )
for (nclust in c(2:15)){
print(nclust)
rf.cluster  <- cutree( hclust.rf, nclust) #corto los arboles
sil <- silhouette (rf.cluster,dist_mtrx) # or use your cluster vector
sil_avg <- summary(sil)[[4]]
row <- c("imputer" = imputer_i, "ncluster"=nclust, "avg_sil" = sil_avg)
df_sil <-  rbind(df_sil, data.frame(t(row)) )
}
rf_list[[imputer_i]] = modelo
dist_list[[imputer_i]] = dist_mtrx
hclust_list[[imputer_i]] = hclust.rf
corr_coph[[imputer_i]] = cor(cophenetic(hclust.rf), dist_mtrx)
# sil_list[[imputer_i]] = df_sil
df_corr_coph[i, ] <- c(imputer_i  , cor(cophenetic(hclust.rf), dist_mtrx) )
i = i +1
# df_sil <- data.frame(matrix(ncol = 3, nrow = 0))
# colnames(df_sil) <- c("imputer", "ncluster", "avg_sil")
}
df_corr_coph
c(imputer_i  , cor(cophenetic(hclust.rf), dist_mtrx)
c(imputer_i  , cor(cophenetic(hclust.rf), dist_mtrx) )
df_corr_coph <- data.frame(imputer = numeric(0),    # Create empty data frame
corr_coph = numeric(0) )
i = 1
df_sil <- data.frame(matrix(ncol = 3, nrow = 0))
colnames(df_sil) <- c("imputer", "ncluster", "avg_sil")
df_corr_coph <- data.frame(imputer = numeric(0),    # Create empty data frame
corr_coph = numeric(0) )
i = 1
df_sil <- data.frame(matrix(ncol = 3, nrow = 0))
colnames(df_sil) <- c("imputer", "ncluster", "avg_sil")
for(imputer_i in unique(data$imputer)){
print(imputer_i)
modelo  <- randomForest( x= data %>%
filter(imputer== imputer_i)  %>%
select(-imputer)
,
y= NULL,
ntree= 100, #se puede aumentar a 10000
proximity= TRUE,
oob.prox = TRUE )
#genero los clusters jerarquicos
dist_mtrx <-  as.dist ( 1.0 - modelo$proximity) #distancia = 1.0 - proximidad
hclust.rf  <- hclust( dist_mtrx,
method= "ward.D2" )
for (nclust in c(2:15)){
print(nclust)
rf.cluster  <- cutree( hclust.rf, nclust) #corto los arboles
sil <- silhouette (rf.cluster,dist_mtrx) # or use your cluster vector
sil_avg <- summary(sil)[[4]]
row <- c("imputer" = imputer_i, "ncluster"=nclust, "avg_sil" = sil_avg)
df_sil <-  rbind(df_sil, data.frame(t(row)) )
}
rf_list[[imputer_i]] = modelo
dist_list[[imputer_i]] = dist_mtrx
hclust_list[[imputer_i]] = hclust.rf
corr_coph[[imputer_i]] = cor(cophenetic(hclust.rf), dist_mtrx)
# sil_list[[imputer_i]] = df_sil
df_corr_coph[nrow(df_corr_coph)+1, ] <- c(imputer_i  , cor(cophenetic(hclust.rf), dist_mtrx) )
i = i +1
# df_sil <- data.frame(matrix(ncol = 3, nrow = 0))
# colnames(df_sil) <- c("imputer", "ncluster", "avg_sil")
}
df_corr_coph
# corr cofenética
write_delim(df_corr_coph %>% arrange(-corr_coph), file = "results/cophenetic_corr.csv", delim = "&")
df_corr_coph %>% arrange(-corr_coph)
df_corr_coph
# corr cofenética
write_delim(df_corr_coph %>%
as.data.frame %>% arrange(-corr_coph), file = "results/cophenetic_corr.csv", delim = "&")
summary(df_corr_coph)
# corr cofenética
df_corr_coph <- df_corr_coph %>%
mutate(corr_coph = as.double(corr_coph)) %>%
arrange(-corr_coph)
write_delim(df_corr_coph, file = "results/cophenetic_corr.csv", delim = "&")
df_corr_coph
